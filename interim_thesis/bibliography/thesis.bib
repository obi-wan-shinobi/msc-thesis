@article{CC90,
  author        = "E. J. Chikofsky and J. H. Cross",
  title         = "Reverse Engineering and Design Recovery: A Taxonomy",
  journal       = "IEEE Software",
  year          = 1990,
  volume        = 7,
  number        = 1,
  pages         = "13--17",
  publisher     = "IEEE Computer Society Press"
}
@book{Whe95,
  author        = "Colin Wheildon",
  title         = "Type \& Layout",
  publisher     = "Strathmore Press",
  note          = "(ISBN 0 9624891 5 8)",
  year          = 1995
}
% ------------------------------------------------------
@misc{jacot2018ntk,
  title         = {Neural Tangent Kernel: Convergence and Generalization in Neural Networks},
  author        = {Arthur Jacot and Franck Gabriel and Cl\'{e}ment Hongler},
  year          = {2020},
  eprint        = {1806.07572},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/1806.07572}
}
@article{lee2020wide,
  title         = {Wide neural networks of any depth evolve as linear models under gradient descent \textasteriskcentered},
  volume        = {2020},
  issn          = {1742-5468},
  url           = {http://dx.doi.org/10.1088/1742-5468/abc62b},
  doi           = {10.1088/1742-5468/abc62b},
  number        = {12},
  journal       = {Journal of Statistical Mechanics: Theory and Experiment},
  publisher     = {IOP Publishing},
  author        = {Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel S and Bahri, Yasaman and Novak, Roman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
  year          = {2020},
  month         = dec,
  pages         = {124002}
}
@inbook{neal1996priors,
  author        = "Neal, Radford M.",
  title         = "Priors for Infinite Networks",
  booktitle     = "Bayesian Learning for Neural Networks",
  year          = "1996",
  publisher     = "Springer New York",
  address       = "New York, NY",
  pages         = "29--53",
  abstract      = "In this chapter, I show that priors over network parameters can be defined in such a way that the corresponding priors over functions computed by the network reach reasonable limits as the number of hidden units goes to infinity. When using such priors,there is thus no need to limit the size of the network in order to avoid ``overfitting''. The infinite network limit also provides insight into the properties of different priors. A Gaussian prior for hidden-to-output weights results in a Gaussian process prior for functions,which may be smooth, Brownian, or fractional Brownian. Quite different effects can be obtained using priors based on non-Gaussian stable distributions. In networks with more than one hidden layer, a combination of Gaussian and non-Gaussian priors appears most interesting.",
  isbn          = "978-1-4612-0745-0",
  doi           = "10.1007/978-1-4612-0745-0_2",
  url           = "https://doi.org/10.1007/978-1-4612-0745-0_2"
}
@misc{haninNica2019finite,
  title         = {Finite Depth and Width Corrections to the Neural Tangent Kernel},
  author        = {Boris Hanin and Mihai Nica},
  year          = {2019},
  eprint        = {1909.05989},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/1909.05989}
}
@article{krizhevsky2012imagenet,
  title         = {Imagenet classification with deep convolutional neural networks},
  author        = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal       = {Advances in neural information processing systems},
  volume        = {25},
  year          = {2012}
}
@misc{choromanska2015losssurfacesmultilayernetworks,
  title         = {The Loss Surfaces of Multilayer Networks},
  author        = {Anna Choromanska and Mikael Henaff and Michael Mathieu and G\'{e}rard Ben Arous and Yann LeCun},
  year          = {2015},
  eprint        = {1412.0233},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/1412.0233}
}
@inproceedings{he2016deep,
  title         = {Deep residual learning for image recognition},
  author        = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle     = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages         = {770--778},
  year          = {2016}
}
@misc{zhang2017understandingdeeplearningrequires,
  title         = {Understanding deep learning requires rethinking generalization},
  author        = {Chiyuan Zhang and Samy Bengio and Moritz Hardt and Benjamin Recht and Oriol Vinyals},
  year          = {2017},
  eprint        = {1611.03530},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/1611.03530}
}
@misc{vaswani2023attentionneed,
  title         = {Attention Is All You Need},
  author        = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  year          = {2023},
  eprint        = {1706.03762},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/1706.03762}
}
@article{Belkin_2019,
  title         = {Reconciling modern machine-learning practice and the classical bias–variance trade-off},
  volume        = {116},
  issn          = {1091-6490},
  url           = {http://dx.doi.org/10.1073/pnas.1903070116},
  doi           = {10.1073/pnas.1903070116},
  number        = {32},
  journal       = {Proceedings of the National Academy of Sciences},
  publisher     = {Proceedings of the National Academy of Sciences},
  author        = {Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  year          = {2019},
  month         = jul,
  pages         = {15849–15854}
}
@misc{arora2019finegrainedanalysisoptimizationgeneralization,
  title         = {Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks},
  author        = {Sanjeev Arora and Simon S. Du and Wei Hu and Zhiyuan Li and Ruosong Wang},
  year          = {2019},
  eprint        = {1901.08584},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/1901.08584}
}
@article{Mei2018meanfieldview,
  title         = {A mean field view of the landscape of two-layer neural networks},
  volume        = {115},
  issn          = {1091-6490},
  url           = {http://dx.doi.org/10.1073/pnas.1806579115},
  doi           = {10.1073/pnas.1806579115},
  number        = {33},
  journal       = {Proceedings of the National Academy of Sciences},
  publisher     = {Proceedings of the National Academy of Sciences},
  author        = {Mei, Song and Montanari, Andrea and Nguyen, Phan-Minh},
  year          = {2018},
  month         = jul
}
@misc{chizat2018globalconvergencegradientdescent,
  title         = {On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport},
  author        = {Lenaic Chizat and Francis Bach},
  year          = {2018},
  eprint        = {1805.09545},
  archiveprefix = {arXiv},
  primaryclass  = {math.OC},
  url           = {https://arxiv.org/abs/1805.09545}
}
@article{chizat2022infinitewidthlimitdeeplinear,
  author        = {Chizat, L\'{e}na\"{\i}c and Colombo, Maria and Fern\'{a}ndez-Real, Xavier and Figalli, Alessio},
  title         = {Infinite-width limit of deep linear neural networks},
  journal       = {Communications on Pure and Applied Mathematics},
  volume        = {77},
  number        = {10},
  pages         = {3958--4007},
  doi           = {https://doi.org/10.1002/cpa.22200},
  url           = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpa.22200},
  eprint        = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpa.22200},
  abstract      = {Abstract This paper studies the infinite-width limit of deep linear neural networks (NNs) initialized with random parameters. We obtain that, when the number of parameters diverges, the training dynamics converge (in a precise sense) to the dynamics obtained from a gradient descent on an infinitely wide deterministic linear NN. Moreover, even if the weights remain random, we get their precise law along the training dynamics, and prove a quantitative convergence result of the linear predictor in terms of the number of parameters. We finally study the continuous-time limit obtained for infinitely wide linear NNs and show that the linear predictors of the NN converge at an exponential rate to the minimal \$\ell \_2\$-norm minimizer of the risk.},
  year          = {2024}
}
@misc{bartolucci2021understandingneuralnetworksreproducing,
  title         = {Understanding neural networks with reproducing kernel Banach spaces},
  author        = {Francesca Bartolucci and Ernesto De Vito and Lorenzo Rosasco and Stefano Vigogna},
  year          = {2021},
  eprint        = {2109.09710},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML},
  url           = {https://arxiv.org/abs/2109.09710}
}
@misc{bietti2019inductivebiasneuraltangent,
  title         = {On the Inductive Bias of Neural Tangent Kernels},
  author        = {Alberto Bietti and Julien Mairal},
  year          = {2019},
  eprint        = {1905.12173},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML},
  url           = {https://arxiv.org/abs/1905.12173}
}
@article{Rotskoff_2022,
  title         = {Trainability and Accuracy of Artificial Neural Networks: An Interacting Particle System Approach},
  volume        = {75},
  issn          = {1097-0312},
  url           = {http://dx.doi.org/10.1002/cpa.22074},
  doi           = {10.1002/cpa.22074},
  number        = {9},
  journal       = {Communications on Pure and Applied Mathematics},
  publisher     = {Wiley},
  author        = {Rotskoff, Grant and Vanden-Eijnden, Eric},
  year          = {2022},
  month         = jul,
  pages         = {1889–1935}
}
@misc{nguyen2019meanfieldlimitlearning,
  title         = {Mean Field Limit of the Learning Dynamics of Multilayer Neural Networks},
  author        = {Phan-Minh Nguyen},
  year          = {2019},
  eprint        = {1902.02880},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/1902.02880}
}
@misc{saxe2014exactsolutionsnonlineardynamics,
  title         = {Exact solutions to the nonlinear dynamics of learning in deep linear neural networks},
  author        = {Andrew M. Saxe and James L. McClelland and Surya Ganguli},
  year          = {2014},
  eprint        = {1312.6120},
  archiveprefix = {arXiv},
  primaryclass  = {cs.NE},
  url           = {https://arxiv.org/abs/1312.6120}
}
@misc{tu2024mixeddynamicslinearnetworks,
  title         = {Mixed Dynamics In Linear Networks: Unifying the Lazy and Active Regimes},
  author        = {Zhenfeng Tu and Santiago Aranguri and Arthur Jacot},
  year          = {2024},
  eprint        = {2405.17580},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2405.17580}
}
@misc{araujo2019meanfieldlimitcertaindeep,
  title         = {A mean-field limit for certain deep neural networks},
  author        = {Dyego Ara\'{u}jo and Roberto I. Oliveira and Daniel Yukimura},
  year          = {2019},
  eprint        = {1906.00193},
  archiveprefix = {arXiv},
  primaryclass  = {math.ST},
  url           = {https://arxiv.org/abs/1906.00193}
}
@misc{sirignano2019meanfieldanalysisneural,
  title         = {Mean Field Analysis of Neural Networks: A Law of Large Numbers},
  author        = {Justin Sirignano and Konstantinos Spiliopoulos},
  year          = {2019},
  eprint        = {1805.01053},
  archiveprefix = {arXiv},
  primaryclass  = {math.PR},
  url           = {https://arxiv.org/abs/1805.01053}
}
@misc{nguyen2023rigorousframeworkmeanfield,
  title         = {A Rigorous Framework for the Mean Field Limit of Multilayer Neural Networks},
  author        = {Phan-Minh Nguyen and Huy Tuan Pham},
  year          = {2023},
  eprint        = {2001.11443},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2001.11443}
}
@article{vadaranjan1958convergenceofprobabilitydistribution,
  issn          = {00364452},
  url           = {http://www.jstor.org/stable/25048365},
  author        = {V. S. Varadarajan},
  journal       = {Sankhy\={a}: The Indian Journal of Statistics (1933-1960)},
  number        = {1/2},
  pages         = {23--26},
  publisher     = {Springer},
  title         = {On the Convergence of Sample Probability Distributions},
  urldate       = {2025-12-24},
  volume        = {19},
  year          = {1958}
}
@phdthesis{golikov2025deepneuralnetworkslargewidthbehavior,
  address       = {Lausanne},
  title         = {Deep Neural Networks: Large-Width Behavior and Generalization Bounds},
  url           = {https://infoscience.epfl.ch/handle/20.500.14299/247097},
  doi           = {10.5075/epfl-thesis-11255},
  school        = {EPFL},
  author        = {Golikov, Evgenii},
  year          = {2025},
  keywords      = {machine learning \vert{} neural networks \vert{} deep learning \vert{} deep learning theory \vert{} infinitely-wide networks \vert{} tensor programs \vert{} neural tangent kernel \vert{} tail bounds \vert{} loss landscape \vert{} generalization},
  language      = {en}
}
@misc{rahaman2019spectralbiasneuralnetworks,
  title         = {On the Spectral Bias of Neural Networks},
  author        = {Nasim Rahaman and Aristide Baratin and Devansh Arpit and Felix Draxler and Min Lin and Fred A. Hamprecht and Yoshua Bengio and Aaron Courville},
  year          = {2019},
  eprint        = {1806.08734},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML},
  url           = {https://arxiv.org/abs/1806.08734}
}
@misc{bowman2022spectralbiasoutsidetraining,
  title         = {Spectral Bias Outside the Training Set for Deep Networks in the Kernel Regime},
  author        = {Benjamin Bowman and Guido Montufar},
  year          = {2022},
  eprint        = {2206.02927},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML},
  url           = {https://arxiv.org/abs/2206.02927}
}
@misc{cao2020understandingspectralbiasdeep,
  title         = {Towards Understanding the Spectral Bias of Deep Learning},
  author        = {Yuan Cao and Zhiying Fang and Yue Wu and Ding-Xuan Zhou and Quanquan Gu},
  year          = {2020},
  eprint        = {1912.01198},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/1912.01198}
}
@misc{basri2020frequencybiasneuralnetworks,
  title         = {Frequency Bias in Neural Networks for Input of Non-Uniform Density},
  author        = {Ronen Basri and Meirav Galun and Amnon Geifman and David Jacobs and Yoni Kasten and Shira Kritchman},
  year          = {2020},
  eprint        = {2003.04560},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2003.04560}
}
