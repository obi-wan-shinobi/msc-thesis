\section{Finite Depth/Width Corrections to the NTK (Hanin--Nica)}\label{subsec:finite-dw-ntk}

The classical NTK result with \emph{fixed} depth and width $\to\infty$ yields a deterministic, fixed kernel throughout training \citep{jacot2018ntk,lee2020wide}. In contrast, \citet{haninNica2019finite} analyze fully--connected ReLU networks at \emph{finite} depth and width and show that when depth $d$ and widths $\{n_\ell\}$ grow \emph{together}, the NTK exhibits substantial stochasticity at initialization and evolves non--trivially during training.

\paragraph{Setup.}
Let the network have input dimension $n_0$, hidden widths $n_1,\dots,n_{d-1}$, output dimension $n_d=1$, ReLU activations, zero biases at init (but biases are trainable), and standard variance--preserving scalings. Define the ``inverse temperature''
\[
	\beta \;:=\; \sum_{\ell=1}^{d-1} \frac{1}{n_\ell}\,,\qquad\text{(equal widths $n_\ell=n$ give $\beta=d/n$).}
\]

\paragraph{Fluctuations of the NTK at initialization.}
Denote the (on--diagonal) NTK by $K_N(x,x)$ for input $x$. Hanin--Nica prove
\[
	\mathbb{E}\!\big[K_N(x,x)\big] \;=\; d\!\left(\tfrac12 + \frac{\|x\|_2^2}{n_0}\right),
\]
and show that the normalized second moment scales as
\[
	\frac{\mathbb{E}\!\big[K_N(x,x)^2\big]}{\mathbb{E}\!\big[K_N(x,x)\big]^2}
	\;\simeq\; \exp\!\big(5\,\beta\big)\,\bigl(1 + O(\textstyle\sum_\ell n_\ell^{-2})\bigr).
\]
In particular, for $n_\ell=n$ this ratio is $\simeq \exp(5d/n)$, so when $d/n$ is bounded away from $0$ the standard deviation is of the same order as the mean: the NTK is \emph{not} concentrated (hence not deterministic) even if $d,n\to\infty$ jointly with $d/n=\Theta(1)$.

\paragraph{Training--time evolution at initialization.}
For squared loss and a single--example SGD step on $x$, the mean update of $K_N(x,x)$ at $t=0$ satisfies
\[
	\frac{\mathbb{E}\,[\Delta K_N(x,x)]}{\mathbb{E}[K_N(x,x)]}
	\;\asymp\; \frac{d\,\beta}{n_0}\,\exp(5\beta)\,\Bigl(1+O(\textstyle\sum_\ell n_\ell^{-2})\Bigr),
\]
which, for equal widths, becomes $\asymp \tfrac{d^2}{n\,n_0}\exp(5d/n)$. Thus, unlike the fixed--depth infinite--width setting, the NTK generically \emph{evolves} (data--dependently) when depth and width co--scale.

\begin{remark}[Weak feature learning regime]
	The results suggest a regime with $0<\beta\ll 1$ (e.g.\ $0<d/n\ll 1$) where training remains numerically stable while $K_N$ still evolves, enabling \emph{weak} feature learning beyond the strictly lazy NTK limit. This gives a concrete knob ($\beta$) to interpolate between kernel--like behavior and feature adaptation.
\end{remark}

\paragraph{Notes}
\begin{itemize}
	\item The constant--kernel ODE in \S\ref{sec:ntk-formalization} (\emph{gradient flow under fixed $K$}) exactly matches the fixed--depth, infinite--width limit. When $d$ and $n$ co--scale, $K_t$ becomes stochastic and time--varying, so the ODE becomes
	      \[
		      \frac{d}{dt} f_t \;=\; -\,K_t\,(f_t - y),\qquad K_t\ \text{random and evolving},
	      \]
	      with fluctuations and drift controlled by $\beta$.
	\item Practically, this could help explain empirical gaps between NTK predictions and real networks, and motivates experiments in the small--$\beta$ region to observe ``weak'' feature learning.
\end{itemize}
