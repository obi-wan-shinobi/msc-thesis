@misc{jacot2018ntk,
  title         = {Neural Tangent Kernel: Convergence and Generalization in Neural Networks},
  author        = {Arthur Jacot and Franck Gabriel and Cl\'{e}ment Hongler},
  year          = {2020},
  eprint        = {1806.07572},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/1806.07572}
}
@article{lee2020wide,
  title         = {Wide neural networks of any depth evolve as linear models under gradient descent \textasteriskcentered},
  volume        = {2020},
  issn          = {1742-5468},
  url           = {http://dx.doi.org/10.1088/1742-5468/abc62b},
  doi           = {10.1088/1742-5468/abc62b},
  number        = {12},
  journal       = {Journal of Statistical Mechanics: Theory and Experiment},
  publisher     = {IOP Publishing},
  author        = {Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel S and Bahri, Yasaman and Novak, Roman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
  year          = {2020},
  month         = dec,
  pages         = {124002}
}
@inbook{neal1996priors,
  author        = "Neal, Radford M.",
  title         = "Priors for Infinite Networks",
  booktitle     = "Bayesian Learning for Neural Networks",
  year          = "1996",
  publisher     = "Springer New York",
  address       = "New York, NY",
  pages         = "29--53",
  abstract      = "In this chapter, I show that priors over network parameters can be defined in such a way that the corresponding priors over functions computed by the network reach reasonable limits as the number of hidden units goes to infinity. When using such priors,there is thus no need to limit the size of the network in order to avoid ``overfitting''. The infinite network limit also provides insight into the properties of different priors. A Gaussian prior for hidden-to-output weights results in a Gaussian process prior for functions,which may be smooth, Brownian, or fractional Brownian. Quite different effects can be obtained using priors based on non-Gaussian stable distributions. In networks with more than one hidden layer, a combination of Gaussian and non-Gaussian priors appears most interesting.",
  isbn          = "978-1-4612-0745-0",
  doi           = "10.1007/978-1-4612-0745-0_2",
  url           = "https://doi.org/10.1007/978-1-4612-0745-0_2"
}
@misc{haninNica2019finite,
  title         = {Finite Depth and Width Corrections to the Neural Tangent Kernel},
  author        = {Boris Hanin and Mihai Nica},
  year          = {2019},
  eprint        = {1909.05989},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/1909.05989}
}
@misc{bai2020linearizationquadratichigherorderapproximation,
  title         = {Beyond Linearization: On Quadratic and Higher-Order Approximation of Wide Neural Networks},
  author        = {Yu Bai and Jason D. Lee},
  year          = {2020},
  eprint        = {1910.01619},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/1910.01619}
}
@misc{soudry2024implicit,
  title         = {The Implicit Bias of Gradient Descent on Separable Data},
  author        = {Daniel Soudry and Elad Hoffer and Mor Shpigel Nacson and Suriya Gunasekar and Nathan Srebro},
  year          = {2024},
  eprint        = {1710.10345},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML},
  url           = {https://arxiv.org/abs/1710.10345}
}
@misc{gunasekar2018implicit,
  title         = {Implicit Regularization in Matrix Factorization},
  author        = {Suriya Gunasekar and Blake Woodworth and Srinadh Bhojanapalli and Behnam Neyshabur and Nathan Srebro},
  year          = {2017},
  eprint        = {1705.09280},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML},
  url           = {https://arxiv.org/abs/1705.09280}
}
@article{bartolucci2023rkbs,
  title         = {Understanding neural networks with reproducing kernel Banach spaces},
  author        = {Bartolucci, Francesca and De Vito, Ernesto and Rosasco, Lorenzo and Vigogna, Stefano},
  journal       = {Applied and Computational Harmonic Analysis},
  volume        = {62},
  pages         = {194--236},
  year          = {2023},
  publisher     = {Elsevier},
  doi           = {10.1016/j.acha.2022.08.006},
  url           = {https://www.sciencedirect.com/science/article/pii/S1063520322000768}
}
